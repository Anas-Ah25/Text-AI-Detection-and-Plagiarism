{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6890527,"sourceType":"datasetVersion","datasetId":3942644}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture \npip install transformers datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:29:02.656716Z","iopub.execute_input":"2025-05-10T06:29:02.656990Z","iopub.status.idle":"2025-05-10T06:29:07.208954Z","shell.execute_reply.started":"2025-05-10T06:29:02.656963Z","shell.execute_reply":"2025-05-10T06:29:07.207776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install --upgrade transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:29:07.210693Z","iopub.execute_input":"2025-05-10T06:29:07.210991Z","iopub.status.idle":"2025-05-10T06:29:18.107539Z","shell.execute_reply.started":"2025-05-10T06:29:07.210962Z","shell.execute_reply":"2025-05-10T06:29:18.106775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cd /kaggle/input/daigt-proper-train-dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:29:41.709244Z","iopub.execute_input":"2025-05-10T06:29:41.709895Z","iopub.status.idle":"2025-05-10T06:29:41.728522Z","shell.execute_reply.started":"2025-05-10T06:29:41.709872Z","shell.execute_reply":"2025-05-10T06:29:41.727953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======= Load Libraries =======\n# ======= Load DAIGET Dataset =======\ndf = pd.read_csv(\"train_drcat_01.csv\")\nfor file in [\"train_drcat_02.csv\", \"train_drcat_03.csv\", \"train_drcat_04.csv\"]:\n    df = pd.concat([df, pd.read_csv(file)], ignore_index=True)\n\n# Binary label: 1 = AI, 0 = human\ndf[\"label\"] = df[\"label\"].apply(lambda x: 1 if x >= 0.95 else 0)\n\n# Balance: 1000 per class\nsampled_df = pd.concat([\n    df[df[\"label\"] == 0].sample(n=10, random_state=42),\n    df[df[\"label\"] == 1].sample(n=10, random_state=42)\n]).sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Split 80/20 train/val\ntrain_df = sampled_df.sample(frac=0.8, random_state=42)\nval_df = sampled_df.drop(train_df.index)\n\n# ======= Load Deepfake Dataset (OOD Test) =======\ndeepfake_ds = load_dataset(\"yaful/MAGE\", split=\"train\")\nsampled_deepfake_df = pd.DataFrame(deepfake_ds.select(range(100)))\nsampled_deepfake_df[\"label\"] = 1  # All assumed AI-generated","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:29:41.729267Z","iopub.execute_input":"2025-05-10T06:29:41.729590Z","iopub.status.idle":"2025-05-10T06:30:24.982885Z","shell.execute_reply.started":"2025-05-10T06:29:41.729571Z","shell.execute_reply":"2025-05-10T06:30:24.982340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom transformers import AlbertTokenizer, AlbertForSequenceClassification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom datasets import load_dataset\nimport numpy as np\nimport random\nimport pandas as pd\n\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"labels\": torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load and sample datasets\ndef load_and_sample_data():\n    # Load DAIGT dataset from Kaggle CSV\n\n    daigt_df = pd.read_csv(\"/kaggle/input/daigt-proper-train-dataset/train_drcat_01.csv\")\n    for file in [\"train_drcat_02.csv\", \"train_drcat_03.csv\", \"train_drcat_04.csv\"]:\n        daigt_df = pd.concat([df, pd.read_csv(file)], ignore_index=True)\n\n    # Binary label: 1 = AI, 0 = human\n    daigt_df[\"label\"] = daigt_df[\"label\"].apply(lambda x: 1 if x >= 0.95 else 0)\n\n    class_0 = daigt_df[daigt_df[\"label\"] == 0]\n    class_1 = daigt_df[daigt_df[\"label\"] == 1]\n    \n    if len(class_0) < 40000 or len(class_1) < 40000:\n        raise ValueError(f\"Not enough samples: class 0 has {len(class_0)}, class 1 has {len(class_1)}. Need 2000 each.\")\n        \n    # Balance: 1000 per class\n    daigt_sample = pd.concat([\n    daigt_df[daigt_df[\"label\"] == 0].sample(n=40000, random_state=42),\n    daigt_df[daigt_df[\"label\"] == 1].sample(n=40000, random_state=42)\n            ]).sample(frac=1, random_state=42).reset_index(drop=True)\n    \n    # Sample 2000 examples per class (4000 total)\n    # class_0 = daigt_df[daigt_df[\"label\"] == 0]\n    # class_1 = daigt_df[daigt_df[\"label\"] == 1]\n    \n    \n    \n    # # class_0 = class_0.sample(n=7000, random_state=42)\n    # # class_1 = class_1.sample(n=7000, random_state=42)\n    # # daigt_sample = pd.concat([class_0, class_1])\n    daigt_texts = daigt_sample[\"text\"].tolist()\n    daigt_labels = daigt_sample[\"label\"].tolist()\n\n    # # Load yaful/MAGE dataset for OOV sample\n    # try:\n    #     mage_dataset = load_dataset(\"yaful/MAGE\", split=\"train\")\n    # except Exception as e:\n    #     print(f\"Error loading MAGE: {e}\")\n    #     raise Exception(\"Ensure you have access to yaful/MAGE.\")\n    # mage_df = pd.DataFrame(mage_dataset)\n    \n    # # Sample 100 OOV examples\n    # class_0_mage = mage_df[mage_df[\"label\"] == 0]\n    # class_1_mage = mage_df[mage_df[\"label\"] == 1]\n    # class_0_mage = class_0_mage.sample(n=2000, random_state=42)\n    # class_1_mage = class_0_mage.sample(n=2000, random_state=42)\n    # mage_sample = pd.concat([class_0_mage, class_1_mage])\n    # mage_texts = mage_sample[\"text\"].tolist()\n    # mage_labels = [1 - label for label in mage_sample[\"label\"].tolist()]\n    # # Combine datasets\n    # texts = daigt_texts + mage_texts\n    # labels = daigt_labels + mage_labels\n\n    return daigt_texts, daigt_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:39:27.906698Z","iopub.execute_input":"2025-05-10T11:39:27.907455Z","iopub.status.idle":"2025-05-10T11:39:27.913775Z","shell.execute_reply.started":"2025-05-10T11:39:27.907430Z","shell.execute_reply":"2025-05-10T11:39:27.912902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# mage_dataset = load_dataset(\"yaful/MAGE\", split=\"train\")\n# mage_df = pd.DataFrame(mage_dataset)\n# mage_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:39:30.201112Z","iopub.execute_input":"2025-05-10T11:39:30.201824Z","iopub.status.idle":"2025-05-10T11:39:30.204808Z","shell.execute_reply.started":"2025-05-10T11:39:30.201797Z","shell.execute_reply":"2025-05-10T11:39:30.204150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load MAGE test set\ndef load_mage_test_data(tokenizer, sample_size=1000):\n    try:\n        mage_dataset = load_dataset(\"yaful/MAGE\", split=\"train\")\n    except Exception as e:\n        print(f\"Error loading MAGE: {e}\")\n        raise Exception(\"Ensure you have access to yaful/MAGE.\")\n    mage_df = pd.DataFrame(mage_dataset)\n    \n    # Sample 500 examples for testing (distinct from OOV samples)\n    mage_test = mage_df.sample(n=sample_size, random_state=43)  # Different seed to avoid overlap\n    texts = mage_test[\"text\"].tolist()\n    labels = mage_test[\"label\"].tolist()\n    return TextDataset(texts, labels, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:39:30.392791Z","iopub.execute_input":"2025-05-10T11:39:30.393007Z","iopub.status.idle":"2025-05-10T11:39:30.397810Z","shell.execute_reply.started":"2025-05-10T11:39:30.392989Z","shell.execute_reply":"2025-05-10T11:39:30.397217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluation function\ndef evaluate_model(model, test_loader):\n    model.eval()\n    predictions = []\n    true_labels = []\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).cpu().numpy()\n            predictions.extend(preds)\n            true_labels.extend(labels.cpu().numpy())\n\n    # Calculate metrics\n    accuracy = accuracy_score(true_labels, predictions)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        true_labels, predictions, average=\"binary\"\n    )\n\n    return {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:39:30.552022Z","iopub.execute_input":"2025-05-10T11:39:30.552214Z","iopub.status.idle":"2025-05-10T11:39:30.557651Z","shell.execute_reply.started":"2025-05-10T11:39:30.552198Z","shell.execute_reply":"2025-05-10T11:39:30.557048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main function to fine-tune ALBERT\ndef main():\n    # Load tokenizer and model\n    tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n    model = AlbertForSequenceClassification.from_pretrained(\n        \"albert-base-v2\",\n        num_labels=2,\n        hidden_dropout_prob=0.1\n    ).to(device)\n\n    # Load and preprocess data\n    texts, labels = load_and_sample_data()\n\n    # Train-test split (80:20)\n    train_texts, test_texts, train_labels, test_labels = train_test_split(\n        texts, labels, test_size=0.2, random_state=42, stratify=labels\n    )\n\n    # Create datasets\n    train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n    test_dataset = TextDataset(test_texts, test_labels, tokenizer)\n    mage_test_dataset = load_mage_test_data(tokenizer)\n\n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=16)\n    #mage_test_loader = DataLoader(mage_test_dataset, batch_size=16)\n\n    # Set up optimizer\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n\n    # Training loop\n    num_epochs = 5\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0\n        for batch in train_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n\n    # Evaluate on 80:20 test set\n    test_metrics = evaluate_model(model, test_loader)\n    print(\"\\n80:20 Test Set Evaluation Results:\")\n    print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n    print(f\"Precision: {test_metrics['precision']:.4f}\")\n    print(f\"Recall: {test_metrics['recall']:.4f}\")\n    print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n\n    # # Evaluate on MAGE test set\n    # mage_metrics = evaluate_model(model, mage_test_loader)\n    # print(\"\\nMAGE Test Set Evaluation Results:\")\n    # print(f\"Accuracy: {mage_metrics['accuracy']:.4f}\")\n    # print(f\"Precision: {mage_metrics['precision']:.4f}\")\n    # print(f\"Recall: {mage_metrics['recall']:.4f}\")\n    # print(f\"F1 Score: {mage_metrics['f1']:.4f}\")\n\n    # Save the model\n    model.save_pretrained(\"/kaggle/working/albert_finetuned_model\")\n    tokenizer.save_pretrained(\"/kaggle/working/albert_finetuned_tokenizer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:39:30.980188Z","iopub.execute_input":"2025-05-10T11:39:30.980466Z","iopub.status.idle":"2025-05-10T11:39:30.988894Z","shell.execute_reply.started":"2025-05-10T11:39:30.980447Z","shell.execute_reply":"2025-05-10T11:39:30.988136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:39:31.318405Z","iopub.execute_input":"2025-05-10T11:39:31.318654Z","iopub.status.idle":"2025-05-10T12:47:40.187355Z","shell.execute_reply.started":"2025-05-10T11:39:31.318634Z","shell.execute_reply":"2025-05-10T12:47:40.186690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/ALBERT.zip /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:15:40.558482Z","iopub.execute_input":"2025-05-10T13:15:40.559088Z","iopub.status.idle":"2025-05-10T13:15:46.040520Z","shell.execute_reply.started":"2025-05-10T13:15:40.559064Z","shell.execute_reply":"2025-05-10T13:15:46.039601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install fastapi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T09:43:21.702788Z","iopub.execute_input":"2025-05-10T09:43:21.703378Z","iopub.status.idle":"2025-05-10T09:43:25.210493Z","shell.execute_reply.started":"2025-05-10T09:43:21.703352Z","shell.execute_reply":"2025-05-10T09:43:25.209309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install uvicorn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T09:44:03.291182Z","iopub.execute_input":"2025-05-10T09:44:03.291688Z","iopub.status.idle":"2025-05-10T09:44:06.484193Z","shell.execute_reply.started":"2025-05-10T09:44:03.291655Z","shell.execute_reply":"2025-05-10T09:44:06.483288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install pyngrok","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:19:37.822690Z","iopub.execute_input":"2025-05-10T11:19:37.823026Z","iopub.status.idle":"2025-05-10T11:19:40.788005Z","shell.execute_reply.started":"2025-05-10T11:19:37.823004Z","shell.execute_reply":"2025-05-10T11:19:40.786892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from fastapi import FastAPI\nfrom pydantic import BaseModel\nimport torch\nfrom transformers import AlbertTokenizer, AlbertForSequenceClassification\nimport numpy as np\nimport nest_asyncio\nimport uvicorn\nfrom pyngrok import ngrok\nimport threading\n\n# Apply nest_asyncio to allow nested event loops\nnest_asyncio.apply()\n\n# Initialize FastAPI app\napp = FastAPI()\n\n# Define input data model\nclass TextInput(BaseModel):\n    text: str\n\n# Load fine-tuned model and tokenizer\nmodel_path = \"/kaggle/working/albert_finetuned\"\ntokenizer = AlbertTokenizer.from_pretrained(model_path)\nmodel = AlbertForSequenceClassification.from_pretrained(model_path)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\n# Prediction function\ndef predict_text(text: str):\n    encoding = tokenizer(\n        text,\n        add_special_tokens=True,\n        max_length=128,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    input_ids = encoding[\"input_ids\"].to(device)\n    attention_mask = encoding[\"attention_mask\"].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n        pred_class = np.argmax(probs).item()\n\n    label = \"human\" if pred_class == 0 else \"machine-generated\"\n    confidence = float(probs[pred_class])\n\n    return {\"label\": label, \"confidence\": confidence}\n\n# API endpoint\n@app.post(\"/classify\")\nasync def classify_text(input: TextInput):\n    result = predict_text(input.text)\n    return result\n\n# Function to run Uvicorn server\ndef run_server():\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n# Main execution\nif __name__ == \"__main__\":\n    # Install pyngrok if not already installed\n    try:\n        from pyngrok import ngrok\n    except ImportError:\n        import os\n        os.system(\"pip install pyngrok\")\n        from pyngrok import ngrok\n\n    # Set ngrok authtoken (replace with your authtoken)\n    ngrok.set_auth_token(\"2wtqB90OJlII7ydBPNMSVpLFGx8_3sQFNcNMvZYQXALrS6exJ\")  # Replace with your ngrok authtoken\n\n    # Start ngrok tunnel\n    public_url = ngrok.connect(8000, bind_tls=True)\n    print(f\"Ngrok tunnel started at: {public_url}\")\n\n    # Run Uvicorn server in a separate thread to avoid blocking\n    server_thread = threading.Thread(target=run_server)\n    server_thread.start()\n\n    # Keep the main thread alive\n    try:\n        server_thread.join()\n    except KeyboardInterrupt:\n        print(\"Shutting down server...\")\n        ngrok.disconnect(public_url)\n        ngrok.kill()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:15:12.666140Z","iopub.execute_input":"2025-05-10T11:15:12.666435Z","iopub.status.idle":"2025-05-10T11:15:57.811701Z","shell.execute_reply.started":"2025-05-10T11:15:12.666413Z","shell.execute_reply":"2025-05-10T11:15:57.810540Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/daigt-proper-train-dataset/train_drcat_04.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:07:10.220137Z","iopub.execute_input":"2025-05-10T13:07:10.220673Z","iopub.status.idle":"2025-05-10T13:07:11.592985Z","shell.execute_reply.started":"2025-05-10T13:07:10.220649Z","shell.execute_reply":"2025-05-10T13:07:11.592297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.iloc[44204][\"text\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:08:39.076967Z","iopub.execute_input":"2025-05-10T13:08:39.077221Z","iopub.status.idle":"2025-05-10T13:08:39.082533Z","shell.execute_reply.started":"2025-05-10T13:08:39.077200Z","shell.execute_reply":"2025-05-10T13:08:39.081976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AlbertTokenizer, AlbertForSequenceClassification\nimport numpy as np\n\n# Load fine-tuned model and tokenizer\nmodel_path = \"/kaggle/working/albert_finetuned\"\ntokenizer = AlbertTokenizer.from_pretrained(model_path)\nmodel = AlbertForSequenceClassification.from_pretrained(model_path)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\n# Prediction function\ndef predict_text(text: str):\n    # Tokenize input text\n    encoding = tokenizer(\n        text,\n        add_special_tokens=True,\n        max_length=128,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    input_ids = encoding[\"input_ids\"].to(device)\n    attention_mask = encoding[\"attention_mask\"].to(device)\n\n    # Get model predictions\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n        pred_class = np.argmax(probs).item()\n\n    # Map class to label\n    label = \"machine-generated\" if pred_class == 1 else \"human\"\n    confidence = float(probs[pred_class])\n\n    return {\"label\": label, \"confidence\": confidence}\n\n# Test query\nquery = df_test.iloc[44204][\"text\"]\n# Run prediction\nresult = predict_text(query)\nprint(\"Prediction Result:\")\nprint(f\"Label: {result['label']}\")\nprint(f\"Confidence: {result['confidence']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:09:03.107951Z","iopub.execute_input":"2025-05-10T13:09:03.108454Z","iopub.status.idle":"2025-05-10T13:09:03.314268Z","shell.execute_reply.started":"2025-05-10T13:09:03.108431Z","shell.execute_reply":"2025-05-10T13:09:03.313458Z"}},"outputs":[],"execution_count":null}]}